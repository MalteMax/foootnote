{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e7599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, requests, gzip, urllib.request, time, zipfile, shutil, html, unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c67318",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd03a41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58de83",
   "metadata": {},
   "source": [
    "read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10db664",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('/Users/maltemax/Desktop/files_sec') if f.endswith('zip')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf758f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(unzip_location, file, 'tag.tsv'), 'r') as f:\n",
    "#     lis = [line.split('\\t') for line in f]\n",
    "# d = pd.DataFrame(lis)\n",
    "# tag = pd.DataFrame(lis)\n",
    "# tag.columns = tag.loc[0].tolist()\n",
    "# tag = tag.drop(0)\n",
    "# tag.replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031a0d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e30917219cc40adba9440def3492b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 480899: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:13: DtypeWarning: Columns (35,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sub = pd.read_csv(os.path.join(unzip_location, file, 'sub.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 530291: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 91798: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 602232: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:13: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sub = pd.read_csv(os.path.join(unzip_location, file, 'sub.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 374579: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:13: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sub = pd.read_csv(os.path.join(unzip_location, file, 'sub.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:13: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sub = pd.read_csv(os.path.join(unzip_location, file, 'sub.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 838521: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 754776: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:13: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sub = pd.read_csv(os.path.join(unzip_location, file, 'sub.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:18: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 100862: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 69252: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 582206: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2429272046.py:24: ParserWarning: Skipping line 607590: expected 9 fields, saw 10\n",
      "\n",
      "  tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n"
     ]
    }
   ],
   "source": [
    "files_location = '/Users/maltemax/Desktop/files_sec'\n",
    "unzip_location = '/Users/maltemax/Desktop/temp_unzip'\n",
    "if not os.path.exists(unzip_location):\n",
    "    os.makedirs(unzip_location)\n",
    "\n",
    "collect = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    with zipfile.ZipFile(os.path.join(files_location, file), 'r') as zip_file:\n",
    "        zip_file.extractall(os.path.join(unzip_location, file))\n",
    "    \n",
    "    # sub has all submissions for the given period\n",
    "    sub = pd.read_csv(os.path.join(unzip_location, file, 'sub.tsv'), sep = '\\t')\n",
    "    sub = sub.loc[sub['form'] == '10-K']\n",
    "    sub = sub[['adsh', 'cik', 'name', 'sic', 'period', 'fy', 'filed', 'instance']]\n",
    "    \n",
    "    # num has numerical values, one row for each tagged item\n",
    "    num = pd.read_csv(os.path.join(unzip_location, file, 'num.tsv'), sep = '\\t')\n",
    "    num = num.loc[num['tag'].notna()]\n",
    "    num['tag'] = num['tag'].str.lower()\n",
    "    num = num.loc[num['tag'].str.contains('valuationallowance', flags = re.IGNORECASE)]\n",
    "    \n",
    "    # tag has tag information (official description, custom, etc.)\n",
    "    tag = pd.read_csv(os.path.join(unzip_location, file, 'tag.tsv'), sep = '\\t', on_bad_lines = 'warn')\n",
    "    tag = tag.loc[tag['tag'].notna()]\n",
    "    tag['tag'] = tag['tag'].str.lower()\n",
    "    tag = tag.drop_duplicates('tag')\n",
    "    tag = tag[['tag', 'custom', 'tlabel', 'doc']]\n",
    "    \n",
    "    # taking sub as the basis so that firms with no tags are retained\n",
    "    # add num and tag information\n",
    "    sub = sub.merge(right = num, how = 'left', on = 'adsh')\n",
    "    sub = sub.merge(right = tag, how = 'left', on = 'tag')\n",
    "    \n",
    "    collect.append(sub)\n",
    "    \n",
    "    # delete the unzipped directory\n",
    "    shutil.rmtree(os.path.join(unzip_location, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d4a529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/3824935784.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat(collect, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(collect, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade39b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url_instance'] = ('https://www.sec.gov/Archives/edgar/data/' +\n",
    "                      df['cik'].astype(str) + '/' +\n",
    "                      df['adsh'].str.replace('-', '') + '/' +\n",
    "                      df['instance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aaaa177",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(os.getcwd(), '3_pipeline', '1_intermediate', 'va_data_raw.tsv'),\n",
    "          sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aadab90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save meta file to disk\n",
    "meta = df.drop_duplicates('adsh')[['adsh', 'cik', 'name', 'sic', 'period', 'fy',\n",
    "                                 'filed', 'instance', 'url_instance']]\n",
    "# calculate fiscal\n",
    "meta['period'] = pd.to_datetime(meta['period'], format = '%Y%m%d')\n",
    "meta['fyear'] = np.where(meta['period'].dt.month <= 5,\n",
    "                         meta['period'].dt.year - 1,\n",
    "                         meta['period'].dt.year)\n",
    "meta.to_csv(os.path.join(os.getcwd(), '3_pipeline', '1_intermediate', 'va_data_info.tsv'),\n",
    "            sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9136e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046187f",
   "metadata": {},
   "source": [
    "separate df into:\n",
    "\n",
    "1. obs for which the standard tag VA or a different VA tag is available\n",
    "2. obs which do not feature any tag: do they really have no valuation allowance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6a95deb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/2897716670.py:1: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(os.getcwd(), '3_pipeline', '1_intermediate', 'va_data_raw.tsv'),\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), '3_pipeline', '1_intermediate', 'va_data_raw.tsv'),\n",
    "                 sep = '\\t',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5b4afd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['period'] = pd.to_datetime(df['period'], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bedb676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fiscal\n",
    "df['fyear'] = np.where(df['period'].dt.month <= 5,\n",
    "                       df['period'].dt.year - 1,\n",
    "                       df['period'].dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "05467596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['fyear'].isin(range(2009, 2024 + 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "27f013a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88608"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['adsh'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fd5cf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['deferredtaxassetsvaluationallowance',\n",
    "        'deferredtaxassetsvaluationallowancenoncurrent',\n",
    " 'deferredtaxassetsvaluationallowancecurrent',\n",
    " 'valuationallowanceamount', 'valuationallowance'\n",
    " 'lessvaluationallowance', 'deferredtaxliabilitiesvaluationallowance',\n",
    " 'deferredtaxassetsdomesticvaluationallowance',\n",
    " 'deferredtaxassetsforeignvaluationallowance', 'deferredtaxassetvaluationallowance',\n",
    " 'deferredtaxassetsvaluationallowances', 'valuationallowance1',\n",
    " 'deferredtaxvaluationallowance', 'deferredtaxassetsvaluationallowance1',\n",
    " 'deferredtaxassetsvaluationallowanceforeign',\n",
    " 'deferredtaxassetsvaluationallowancedomestic',\n",
    " 'deferredtaxassetsvaluationallowancefederal',\n",
    " 'deferredtaxassetsvaluationallowancecurrentforeign',\n",
    " 'deferredtaxassetsvaluationallowancecurrentfederal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ccad3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_va = df.loc[df['tag'].isin(tags), 'adsh'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "64b3c4fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60207"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(has_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f4c62816",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found = df.loc[df['tag'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7d45c95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20866"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e08091",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bb5cc",
   "metadata": {},
   "source": [
    "cleaning of: 2. obs for which no tag with string \"valuation allowance\" is in XBRL\n",
    "\n",
    "approach: for these obs, check all tables in tax footnote: only if no table contains the string \"valuation allowance\", assign a zero\n",
    "\n",
    "an example of such a company is here: the 10-K HTML shows that they have a VA (https://www.sec.gov/Archives/edgar/data/1462566/000126246314000593/biolog2013k.htm), but they did not tag it; I can use their XBRL file (https://www.sec.gov/Archives/edgar/data/1462566/000126246314000593/biol-20131231.xml), take out the income tax disclosure tag and check for all tables therein; and then delete if the string \"valuation allowance\" appears\n",
    "\n",
    "\n",
    "*remaining problems:* there are some firms that do not include a detailed schedule of deferred tax assets but mention in the text that they have established a full valuation allowance for DTA. examples:\n",
    "- https://www.sec.gov/Archives/edgar/data/1535079/000100201415000128/tpoi10k-12312014.htm\n",
    "- https://www.sec.gov/ix?doc=/Archives/edgar/data/1852707/000157587224000990/bfyw-20240228.htm\n",
    "- https://www.sec.gov/ix?doc=/Archives/edgar/data/1851612/000119312523087706/d624110d10k.htm\n",
    "\n",
    "**how deal with these?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811af35",
   "metadata": {},
   "source": [
    "check whether 'valuation allowance' appears in tables of tax footnotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2a93f450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217c80ddae5841f8bcfa99a7137024ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "external = '/Volumes/T7 Shield/research_data/us_xbrl_instances'\n",
    "out = []\n",
    "\n",
    "for i, r in tqdm(not_found.iterrows(), total = len(not_found)):\n",
    "    path = os.path.join(external, f\"{r['adsh']}.xml\")\n",
    "    with open(path, 'r') as text_in:\n",
    "        text = text_in.read()\n",
    "        # unescape HTML; otherwise opening HTML elements are encoded\n",
    "        text = html.unescape(text)\n",
    "        soup = BeautifulSoup(text, 'xml')\n",
    "    \n",
    "    # find tax footnote (in virtually all cases, this yields a list of 1 element)\n",
    "    footnotes = soup.find_all(re.compile('^(?:us-gaap:)?IncomeTaxDisclosureTextBlock$', re.I))\n",
    "    \n",
    "    mentions_va = 0\n",
    "    table_count = 0\n",
    "    for footnote in footnotes:\n",
    "        # identify all tables in footnote\n",
    "        tables = footnote.find_all('table')\n",
    "        for table in tables:\n",
    "            table_count += 1\n",
    "            table_text = unidecode.unidecode(table.text)\n",
    "            table_text = table_text.lower()\n",
    "            # following LM (Textual Analysis in Accounting and Finance: A Survey),\n",
    "            # only include tables with at least 10% digits\n",
    "            no_numbers = len(re.findall('\\d', table_text))\n",
    "            no_alphabetical = len(re.findall('[a-z]', table_text))\n",
    "            \n",
    "            # if table without alphabetical characters, continue with next table\n",
    "            if no_alphabetical == 0:\n",
    "                continue\n",
    "            \n",
    "            if (no_numbers / no_alphabetical) > 0.1:\n",
    "                # if table contains the string 'valuation allowance',\n",
    "                # set mentions_va to 1\n",
    "                if re.search('valuation allowance', table_text):\n",
    "                    mentions_va = 1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    out.append((r['adsh'], r['url_instance'], len(footnotes), table_count,  mentions_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "47b9326d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_zeros = pd.DataFrame(out, columns = ['adsh', 'url_instance', 'no_footnotes', 'no_tables', 'mentions_va'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7796bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_zeros.to_csv(os.path.join(os.getcwd(), '3_pipeline', '1_intermediate', 'va_data_check_zeros.tsv'),\n",
    "                   sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db790bc2",
   "metadata": {},
   "source": [
    "read if already executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e8e23d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_zeros = pd.read_csv(os.path.join(os.getcwd(), '3_pipeline', '1_intermediate', 'va_data_check_zeros.tsv'),\n",
    "                          sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7db76",
   "metadata": {},
   "source": [
    "delete observations from `not_found` for which `mentions_va == 1` in `check_zeros` (i.e., observations for which, in any table in the tax footnote, the string 'valuation allowance' does appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c7624f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found = not_found.loc[~not_found['adsh'].isin(check_zeros.loc[check_zeros['mentions_va'] == 1, 'adsh'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff07d90",
   "metadata": {},
   "source": [
    "cleaning of: 1. obs for which the standard tag is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f1cdfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = df.loc[df['tag'].isin(tags)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dccf8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked: missing values should be zero VAs\n",
    "va['value'] = va['value'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f0059e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace observations that have a negative VA amount by its absolute value\n",
    "# CONFIRM THIS: I have checked a couple of these obs and saw that this is just because\n",
    "# the companies report the VA in negative form\n",
    "va['value'] = va['value'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a70a8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['ddate'] = pd.to_datetime(va['ddate'], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a97c57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to entries that are about the current fiscal year\n",
    "# as companies also often report VA of last year\n",
    "va = va.loc[(va['period'].dt.year == va['ddate'].dt.year) &\n",
    "            (va['period'].dt.month == va['ddate'].dt.month)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "107bba55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83793"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd45bf8",
   "metadata": {},
   "source": [
    "reports sometimes contain the same amount twice: once with highest precision in DTA table, once in the text (with lower precision); approach: round all VA amounts to lowest precision and then keep only the first of all duplicate observations\n",
    "\n",
    "note: the SEC file contains unscaled values, so I'm not using the scale parameter, but values with different levels of precision have different leves of rounding to decimal points, which is what I use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d8b2a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace INF (shown as 32767) by zero; INF means that it shows exact amounts, no rounding\n",
    "# which is equal to dcml of 0\n",
    "# some documents also appear to have -32768, which I also set to 0\n",
    "va['dcml'] = np.where((va['dcml'] == 32767) | (va['dcml'] == -32768), 0, va['dcml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9f025c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are only very few cases with a dcml of > 0 (what do they mean?)\n",
    "va = va.loc[va['dcml'] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "80e40f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['min_decimal'] = va.groupby('adsh')['dcml'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7410cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round all amounts to lowest level of precision used in report\n",
    "va['va_min_accuracy'] = np.round(va['value'] / (10 ** abs(va['min_decimal'])), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4aceb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = va.drop_duplicates(subset = ['cik', 'fyear', 'va_min_accuracy'], keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963700fb",
   "metadata": {},
   "source": [
    "\n",
    "check if duplicates do not come from the fact that companies report disaggregated amounts as well as totals; approach: check whether values are sum of all other values, and in that case keep the largest value (which is likely the sum of all others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b8b64a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sum_of_numbers(number, numbers):\n",
    "    if number == (sum(numbers) - number):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4dede48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = va.groupby('adsh')['va_min_accuracy'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a80e2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_sum_of_others = {}\n",
    "\n",
    "for k, v in temp_dict.items():\n",
    "    va_sum_of_others[k] = max([is_sum_of_numbers(number = n, numbers = v) for n in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0d4771cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_sum_of_others = pd.DataFrame(va_sum_of_others, index = [0]).T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "255a22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_sum_of_others.columns = ['adsh', 'va_sum_of_others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6c8fac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = va.merge(right = va_sum_of_others, how = 'left', on = 'adsh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7ca2f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['va_max'] = va.groupby('adsh')['value'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "927d2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['drop'] = np.where((va['va_sum_of_others'] == True) &\n",
    "                      (va['value'] != va['va_max']), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f0e8663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = va.loc[va['drop'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a8c93d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = va.drop(columns = ['min_decimal', 'va_min_accuracy', 'va_max', 'va_sum_of_others', 'drop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687d5ca",
   "metadata": {},
   "source": [
    "check whether duplicate comes from the fact that companies use non-standard tags to differentiate between jurisdictional origin of DTA (and hence, VA):\n",
    "\n",
    "sometimes companies differentiate between the current and noncurrent portions of VA, or domestic vs foreign, or federal vs foreign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e3101ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [('deferredtaxassetsvaluationallowancenoncurrent',\n",
    "           'deferredtaxassetsvaluationallowancecurrent'),\n",
    "          ('deferredtaxassetsdomesticvaluationallowance',\n",
    "           'deferredtaxassetsforeignvaluationallowance'),\n",
    "          ('deferredtaxassetsvaluationallowancecurrentfederal',\n",
    "           'deferredtaxassetsvaluationallowancecurrentforeign')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "53715941",
   "metadata": {},
   "outputs": [],
   "source": [
    "for combo in combos:\n",
    "    # identify reports that have the two tags AND ONLY the two tags\n",
    "    # otherwise I would drop companies that, e.g., only use\n",
    "    # 'deferredtaxassetsvaluationallowancenoncurrent' as their only tag to tag VAs\n",
    "    va['both_present'] = np.where((va['tag'] == combo[0]) |\n",
    "                                  (va['tag'] == combo[1]), 1, 0)\n",
    "    va['both_present'] = va.groupby('adsh')['both_present'].transform('sum')\n",
    "    va['total_tags'] = va.groupby('adsh')['tag'].transform('nunique')\n",
    "    save = va.loc[(va['both_present'] == 2) & (va['total_tags'] == 2)]\n",
    "    \n",
    "    # from va, drop unnecessary columns and drop those for which the tags will be summed up\n",
    "    va = va.drop(columns = ['both_present', 'total_tags'])\n",
    "    va = va.loc[~va['adsh'].isin(save['adsh'])]\n",
    "    \n",
    "    # sum up values and merge back info from 'save'\n",
    "    aggregated_values = save.groupby('adsh')['value'].sum().reset_index()\n",
    "    aggregated_values = aggregated_values.merge(right = save.drop_duplicates('adsh').drop(columns = ['both_present', 'total_tags', 'value']),\n",
    "                                                on = 'adsh', how = 'left')\n",
    "    # overwrite value of tag to be able to idenfity individually-calculated values later\n",
    "    aggregated_values['tag'] = 'CALCULATED_SUM'\n",
    "    \n",
    "    # concatenate va (from which the values that are summed up individually have been dropped above) and\n",
    "    # the individually-summed up values\n",
    "    va = pd.concat([va, aggregated_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de522767",
   "metadata": {},
   "source": [
    "it might be that a company has two VA tags left at this point: 1 that shows a zero (e.g., the current portion of the VA) and a non-zero value (e.g., the non-current portion of the VA). therefore: if there are only two tags left and one of the two is a zero, sum up the amounts manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "27b25a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['va_zero'] = np.where(va['value'] == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b0cfee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['va_zero_sum'] = va.groupby('adsh')['va_zero'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "69e11ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['total_tags'] = va.groupby('adsh')['tag'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "207615ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_values = va.loc[(va['va_zero_sum'] == 1) & (va['total_tags'] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dda5b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_values = aggregated_values.groupby('adsh')['value'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d19f6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = va.loc[va['adsh'].isin(aggregated_values['adsh'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3b0362e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = va.loc[~va['adsh'].isin(aggregated_values['adsh'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e94c3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_values = aggregated_values.merge(right = save.drop_duplicates('adsh').drop(columns = ['va_zero', 'va_zero_sum', 'total_tags', 'value']),\n",
    "                                            on = 'adsh', how = 'left')\n",
    "# overwrite value of tag to be able to idenfity individually-calculated values later\n",
    "aggregated_values['tag'] = 'CALCULATED_SUM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "697f6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = pd.concat([va, aggregated_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5e66a",
   "metadata": {},
   "source": [
    "drop remaining duplicates at cik-fyear level (not keeping any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "da5c33c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "va = va.drop_duplicates(subset = ['cik', 'fyear'], keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a7072a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['value'] = va['value'] / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9cb9c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55648"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0509fd",
   "metadata": {},
   "source": [
    "add a valuation allowance of zero for all reports where the code does not find a VA\n",
    "\n",
    "plus, add indicator for inferred zero (vs. a real zero) >> when code does not find VA, zero is inferred; otherwise real zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4f6e9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['zero_inferred'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "83fac272",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found['tag'] = 'NO_VA_TAG_FOUND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a2b85f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20037"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found['adsh'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3137b0f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/0rcsgf6524x_7ns1b5_sllfm0000gn/T/ipykernel_67656/4105074280.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  va = pd.concat([va, not_found], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "va = pd.concat([va, not_found], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "49e40c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 if zero_inferred is missing (which it is only in the `not_found` dataframe)\n",
    "va['zero_inferred'] = np.where(va['zero_inferred'].isna(), 1, va['zero_inferred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6df7d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only obs from `not_found` have a `value` that is missing; replace with zero\n",
    "# these are observations that don't have the deferredtaxassetsvaluationallowance tag\n",
    "# AND also no other tag that contains 'valuationallowance'\n",
    "va['value'] = np.where(va['value'].isna(), 0, va['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "34c30404",
   "metadata": {},
   "outputs": [],
   "source": [
    "va['value'] = pd.to_numeric(va['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5d549753",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = va[['adsh', 'fyear', 'tag', 'value', 'zero_inferred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a92873cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = va.rename(columns = {'value' : 'va'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "55429cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75685"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b6c01cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "va.to_csv(os.path.join(os.getcwd(), '3_pipeline', '1_intermediate', 'va_data_clean.tsv'),\n",
    "          sep = '\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
